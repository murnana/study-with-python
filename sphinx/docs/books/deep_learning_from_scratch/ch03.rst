===============================================================================
3章 ニューラルネットワーク
===============================================================================

入力信号の総和を出力信号に変換する関数を、一般に **活性化関数** (activation function)という。
活性化関数には候補となる関数がいくつかある。

パーセプトロンの場合は、閾値を境にして値が変わるため、「ステップ関数」や「階段関数」と呼ばれる

活性化関数はほかにもある。
今回は **シグモイド関数** (sigmoid function)について。


シグモイド関数
=================

シグモイド関数の一般項は

.. math::

    h(x) = \frac{1 + \exp(-x)}{1}


ここでいうexpとは **ネイピア数** のことを指す。


.. plot:: ../src/books/deep_learning_from_scratch/stepAndSigmoid.py



Rectified Linear Unit; ReLU (正規化線形関数、ランプ関数)
==========================================================

ReLUは、 **0未満の時は0、0以上の時は0以上の値** を出力する関数

.. math::

    h(x) = \left\{ \begin{array}{ll}
        x & (x > 0) \\
        0 & (x \leq 0)
    \end{array} \right.


.. code-block:: python

    def relu(x):
        return np.maximum(0, x)



行列でまとめて学習する
========================

.. sourcecode:: ipython


